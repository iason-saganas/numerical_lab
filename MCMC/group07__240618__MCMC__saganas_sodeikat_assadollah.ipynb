{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Monte Carlo Markov Chains and Cluster Mass Reconstruction </h1>\n",
    "\n",
    "for questions, problems or suggestions: contact Kerstin Paech (kerstin.paech@physik.lmu.de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:23:52.905051Z",
     "start_time": "2024-06-25T13:23:52.898510Z"
    }
   },
   "outputs": [],
   "source": [
    "#import some modules for math and integrations\n",
    "from scipy import integrate, constants, optimize\n",
    "import numpy as np\n",
    "import timeit\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "import math\n",
    "from styles.matplotlib_style import *\n",
    "figsize = (18,6)\n",
    "figsize_narrow = (9,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> A small primer on classes in Python</h2>\n",
    "\n",
    "In this lab, we will be using classes to manage our data and the functions that operate on the data.\n",
    "__If you need any clarification on this subject, please talk to your supervisor.__\n",
    "\n",
    "The following cell contains the Cluster class. Among other things, it can read the data and calculate model values for a given set of parameters. Familiarize yourself with all the functions and variables that are defined before you proceed with the coding exercise.\n",
    "\n",
    "Before you can use your class in this lab, you need to *instantiate* it (you strictly don't need to instantiate a class, but for this lab we'll ignore this option). For example my_instance = MyClass() creates a new instance of a \n",
    "class and you can access its functions via my_instance.my_function() and variables via my_instance.my_variable. Don't worry too much what an instance acutally is at this point. Just think initialize instead of instantiate if it causes any confusion.\n",
    "\n",
    "As soon as a new instance is created, the `__init__`() function is invoked (if available) to initalise the class and put everyhing into place. As you will see, in our case `__init__`() reads in the data and defines the cosmology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>$\\chi^2$ Calculation</h2>\n",
    "\n",
    "The Cluster class contains all function necessary to calculate the  goodness-of-fit for a given set halo of mass and concentration $(M,c)$. We define the quadratic deviations from the measured shear values:\n",
    "\n",
    "$$ \\chi^2(M,c) = \\sum_i^N \\frac{(\\gamma_\\mathrm{theo, i}(M,c) - \\gamma_\\mathrm{dat, i})^2}{\\sigma^2_\\gamma} $$\n",
    "\n",
    "The theoretical shear values are calculated by assuming a Navarro-Frenk-White (NFW) density profile:\n",
    "\n",
    "$$ \\rho_\\mathrm{NFW}(r) = \\frac{\\delta_c \\rho_c}{(r/r_s)(1 + r/r_s)^2} \\: , $$\n",
    "\n",
    "which leads to\n",
    "\n",
    "$$ \\gamma(x_i) = \\frac{r_s(M,c) \\; \\delta_c(c) \\; \\rho_c(z_\\mathrm{lens})}{\\Sigma_c(z_\\mathrm{lens},z_\\mathrm{source})} g(x_i) \\: , $$\n",
    "\n",
    "where $g(x)$ is an auxiliary function for the dimensionless distance from the lens $x = \\theta/\\theta_s$. \n",
    "\n",
    "Be careful to distinguish $g(x)$ for the cases $x<1$ and $x>1$. When calculating $\\chi^2$. Do not use loops over the whole data array - they are quite slow in python. Numpy arrays however do support fast vectorized operations. A term to look for is \"boolean arrays\".\n",
    "\n",
    "Speed matters here. Try to optimise your $\\chi^2$ calculation, remember that this routine is called several thousand times when used as part of a chain. You can check your execution time with the `%timeit my_function(x,y,..)` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your first task is to complete the Cluster class. If you try to use it as it is, it will produce error messages. From these error messages, work your way back through the code and try to figure out what is still missing. But check out the next few cells before you begin.\n",
    "\n",
    "The likelihood function you will need later on is log_prob - it returns the log-likelihood for the cluster. This is what libraries like emcee (https://emcee.readthedocs.io/en/stable/) expect as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:24:01.994285Z",
     "start_time": "2024-06-25T13:24:01.982452Z"
    }
   },
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    ckms = constants.c/1000. # speed of light in km/s \n",
    "    G = 4.30091e-9 # G in units of Mpc/Msun /(km/s)^2\n",
    "    \n",
    "    def __init__(self, data_filename='halo5.tab', Omega_m=0.27):\n",
    "        \"\"\"\n",
    "        Initialise arrays and variables.\n",
    "        \n",
    "        Optional parameter:\n",
    "        ----------\n",
    "        data_filename : the filename that contains the shear data. Defaults to 'halo5.tab'\n",
    "\n",
    "        \"\"\"\n",
    "        # instance variables\n",
    "        self.shear_err = 0.3 # all data points have the same error\n",
    "        self.zlens = 0.245\n",
    "        self.zsource = 1.0 # all data points have the same redshift\n",
    "\n",
    "        self.Omega_m = Omega_m\n",
    "        self.Omega_L = 1.0 - Omega_m #assumes flatness\n",
    "        self.rho_crit0 = 2.7751973751261264e11 # rho_crit(z=0) in units of h^-1 Msun/ h^-3 Mpc^3\n",
    "\n",
    "        \n",
    "        self.rho_crit_zlens = self.rho_crit0 * self.E(self.zlens)**2\n",
    "        self.r, self.shear = self.read_data(data_filename)\n",
    "        self.set_parameter_limits()\n",
    "        \n",
    "    def read_data(self, data_filename):\n",
    "        \"\"\"\n",
    "        Reads in the cluster dat.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data_filename : the filename that contains the shear data. Defaults to 'halo5.tab'\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        r : radial distance between source and lens\n",
    "        \"\"\"\n",
    "        # read angular distance in arcmin and measured shear signal\n",
    "        theta, shear = np.loadtxt(data_filename, usecols = (3,5), unpack=True)\n",
    "        r = self.angular_distance(self.zlens)*theta*np.pi/180.0/60.0 # physical distance\n",
    "        return r, shear\n",
    "    \n",
    "    def set_parameter_limits(self, logM200_min=14, logM200_max=16, c200_min=2, c200_max=10):\n",
    "        \"\"\"\n",
    "        Set the parameter limits for the cluster model\n",
    "\n",
    "        Optional Parameters:\n",
    "        ----------\n",
    "        logM200_min [default: 10]\n",
    "        logM200_max [default: 20]\n",
    "        c200_min [default: 0.1]\n",
    "        c200_max [default: 10]\n",
    "        \n",
    "        \"\"\"\n",
    "        self.par_min = [logM200_min, c200_min] \n",
    "        self.par_max = [logM200_max, c200_max]\n",
    "\n",
    "    def get_random_parameter(self, logM200_min=None, logM200_max=None, c200_min=None, c200_max=None):\n",
    "        \"\"\"\n",
    "        Return randomly chosen parameters within a given range.\n",
    "\n",
    "        Optional Parameters:\n",
    "        ----------\n",
    "        logM200_min [default: None]\n",
    "        logM200_max [default: None]\n",
    "        c200_min [default: None]\n",
    "        c200_max [default: None]\n",
    "        \n",
    "        \"\"\"\n",
    "        par_min = self.par_min.copy()\n",
    "        par_max = self.par_max.copy()\n",
    "        if logM200_min:\n",
    "            par_min[0] = logM200_min\n",
    "        if logM200_max:\n",
    "            par_max[0] = logM200_max\n",
    "        if c200_min:\n",
    "            par_min[1] = c200_min\n",
    "        if c200_max:\n",
    "            par_max[1] = c200_max\n",
    "        \n",
    "        return np.random.uniform(par_min, par_max)\n",
    "    \n",
    "    def E(self, z):\n",
    "        \"\"\"\n",
    "        Calculates the dimensionless Hubble function.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        z : redshift\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        E(z) : Dimensionless Hubble function E(z)=H(z)/H0\n",
    "        \"\"\"\n",
    "        E = np.sqrt(self.Omega_m*(1.+z)**3 + self.Omega_L)\n",
    "        return E\n",
    "\n",
    "    def angular_distance(self,z):\n",
    "        \"\"\"\n",
    "        Calculates angular diameter distances. Assumes a fixed LCDM cosmology\n",
    "        defined in the Cosmology class.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        z : redshift\n",
    "\n",
    "        Returns :\n",
    "        ----------\n",
    "        d_A(z) : angular diameter distance in Mpc/h\n",
    "        \"\"\"\n",
    "        integral, error = integrate.quad(self.integrand_angular_distance,0.,z,epsrel=1e-4)\n",
    "        d_A = integral * self.ckms*1e-2/(1.+z)\n",
    "        return d_A\n",
    "\n",
    "    def integrand_angular_distance(self,z):\n",
    "        \"\"\"\n",
    "        Integrand for the angular diameter distance calculation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        z : redshift\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        1./E(z) : Inverse dimensionless Hubble function 1./E(z)=H0/H(z)\n",
    "        \"\"\"\n",
    "        return 1./self.E(z)\n",
    "\n",
    "    def Sigma_critical(self,z_lens,z_source):\n",
    "        \"\"\"\n",
    "        Calculates the critical surface density.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        z_lens    : lens redshift\n",
    "        z_source  : source redshift\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Sigmac : critical surface density\n",
    "        \"\"\"\n",
    "        D_source = self.angular_distance(z_source)\n",
    "        D_lens = self.angular_distance(z_lens)\n",
    "        D_lens_source = D_source - (1.0+z_lens)*D_lens/(1.+z_source)    \n",
    "        Sigmac = self.ckms**2*D_source/D_lens/D_lens_source/np.pi/self.G/4.\n",
    "        return Sigmac\n",
    "\n",
    "    def g_less(self,x):\n",
    "        \"\"\"\n",
    "        Auxiliary function g(x) for shear calculation, assumes x < 1.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        x : array_like, dimensionless radial distance between source and lens\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        result : array_like with same shape as x, analytic shear\n",
    "                 integral for a NFW density profile\n",
    "        \"\"\"\n",
    "        arctan = np.arctanh(np.sqrt((1.-x)/(1.+x)))\n",
    "        x_sq = x*x\n",
    "        term1 = 8.*arctan/x_sq/np.sqrt(1.-x_sq) + 4./x_sq*np.log(x/2.) - 2./(x_sq-1)\n",
    "        result = (term1 + 4.*arctan/(x_sq-1.)/np.sqrt(1.-x_sq))\n",
    "        return result\n",
    "\n",
    "    def g_larger(self,x):\n",
    "        \"\"\"\n",
    "        Auxiliary function g(x) for shear calculation, assumes x > 1.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        x : array_like, dimensionless radial distance between source and lens\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        result : array_like with same shape as x, analytic shear\n",
    "                 integral for a NFW density profile\n",
    "        \"\"\"\n",
    "        arctan = np.arctan(np.sqrt((x-1.)/(1.+x)))\n",
    "        x_sq = x*x\n",
    "        term1 = 8.*arctan/x_sq/np.sqrt(x_sq-1.) + 4./x_sq*np.log(x/2.) - 2./(x_sq-1)\n",
    "        result = (term1 + 4.*arctan/(x_sq-1.)**1.5)\n",
    "        return result\n",
    "\n",
    "    def get_model_values(self, M200, c200):\n",
    "        \"\"\"\n",
    "        Calculates the modeled shear values for each of the given data points\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        M200    : cluster mass\n",
    "        c200    : cluster concentration\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        model : array with the modeled shear values\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: \n",
    "        # - calculate scale radius r_s and x = r/r_s\n",
    "        # - call g_less and g_larger depending on the condition if x larger or smaller 1\n",
    "        # - calculate shear\n",
    "        \n",
    "        r_s = self.scale_radius_nfw(M200=M200, c200=c200)\n",
    "        x = self.r / r_s\n",
    "        prefactor = r_s * self.delta_c(c200=c200)*self.rho_crit0 / self.Sigma_critical(\n",
    "                                       z_lens=self.zlens, z_source=self.zsource)\n",
    "        \n",
    "        \n",
    "        model = np.ones(len(x))\n",
    "        model[x < 1] = prefactor * self.g_less(x[x<1])\n",
    "        model[x > 1] = prefactor * self.g_larger(x[x>1])\n",
    "        model[x == 1] = prefactor * (10/3 + 4*np.log(1/2))\n",
    "        \n",
    "        \n",
    "        \"\"\"model = []\n",
    "        for x_i in x:\n",
    "            if x_i<1:\n",
    "                val = prefactor * self.g_less(x=x_i)\n",
    "            elif x_i>1:\n",
    "                val = prefactor * self.g_larger(x=x_i)\n",
    "            else:\n",
    "                val = prefactor * (10/3 + 4*np.log(1/2))\n",
    "            model.append(val)\"\"\"\n",
    "        \n",
    "        # Return gamma\n",
    "        return model\n",
    "    \n",
    "    def chi_sq(self, y_model, y_data, y_err):\n",
    "        \"\"\"\n",
    "        Generic function that calculates the chi square\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        y_model   : values for the model\n",
    "        y_data    : measured data points\n",
    "        y_err     : error of the measured data points\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        chi square for the data and model provided\n",
    "        \"\"\"\n",
    "        # TODO: calculate chi_sq for the data and model vectors given\n",
    "        # It should be a generic function an not \"know\" anything about \n",
    "        # the specifics of the data or the model - that is the role of log_prob\n",
    "        \n",
    "        chi = 1/y_err * (y_model-y_data) \n",
    "\n",
    "        return np.sum(chi**2)\n",
    "    \n",
    "    \n",
    "    def log_prob(self, logM200, c200, normalize=False):\n",
    "        \"\"\"\n",
    "        Calculates the log of the likelihood for a given set of parameters\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        logM200   : log10 of the cluster mass\n",
    "        c200      : cluster concentration\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        log-likelihood of the data\n",
    "        \"\"\"\n",
    "       \n",
    "        # check if parameters are outside of the prior range\n",
    "        if self.outside_param_range([logM200, c200]):\n",
    "            return float('inf')\n",
    "        M200 = 10**logM200\n",
    "        y_model = self.get_model_values(M200, c200)\n",
    "        y_data = self.shear\n",
    "        y_err = self.shear_err\n",
    "        log_prob = self.chi_sq(y_model, y_data, y_err)\n",
    "        if normalize:\n",
    "            ndof = len(y_data)-2\n",
    "            log_prob /= ndof\n",
    "        return log_prob\n",
    "\n",
    "    def outside_param_range(self, p):\n",
    "        \"\"\"\n",
    "        Check if current parameter vector x is in the allowed range, i.e. outside of Prior or not\n",
    "        \"\"\"\n",
    "        for ip, ip_min, ip_max in zip(p, self.par_min, self.par_max):\n",
    "            if ip < ip_min: \n",
    "                return True\n",
    "            if ip > ip_max:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def scale_radius_nfw(self,M200,c200):\n",
    "        \"\"\"\n",
    "        Calculates scale radius of a NFW profile.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        M200 : Mass of the lens enclosed within a radius r200 where the mean density is 200 times\n",
    "               larger than the background matter density\n",
    "        c200 : Halo concentration at radius r200\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        r_s : scale radius\n",
    "        \"\"\"            \n",
    "        r_s = (3.0*M200/800.0/np.pi/self.rho_crit_zlens/c200**3)**(1./3.)\n",
    "        return r_s\n",
    "\n",
    "    def delta_c(self,c200):\n",
    "        \"\"\"\n",
    "        Calculates the central density contrast for a NFW profile.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        c200 : Halo concentration at radius r200\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        delta_c : central density contrast \n",
    "        \"\"\"    \n",
    "        delta_c = 200.*c200**3/(np.log(1.+c200) - c200/(1.+c200))/3.\n",
    "        return delta_c\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start working on our likelihood/chi square, we will do one other thing. The data file is very large and takes a long time to load - which can be a bit annoying when developing/testing your code. Therefore we'll first create a smaller data file that you can use while developing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:24:06.212846Z",
     "start_time": "2024-06-25T13:24:06.185Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_small_data_file(input_filename, output_filename, size):\n",
    "    data = np.loadtxt(input_filename)\n",
    "    idx = np.arange(data.shape[0], dtype=int)\n",
    "    np.random.shuffle(idx)\n",
    "    idx_small = idx[:size]\n",
    "    np.savetxt(output_filename, data[idx_small, :])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to choose how large your small file should be. For checking if everything is running, a small size is fine. But if you would like to check if your likelihood returns sensible numbers, a larger size is better. When you have done all check or you think you need all the data move to the full data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:24:25.337113Z",
     "start_time": "2024-06-25T13:24:16.592773Z"
    }
   },
   "outputs": [],
   "source": [
    "full_size = 627656  # from the header of `halo5.tab`\n",
    "size = full_size\n",
    "filename = 'halo5.tab'\n",
    "filename_small = f'halo5_{size}.tab'\n",
    "create_small_data_file(filename, filename_small, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have generated a small file, get an instance of the cluster class - this contains all the information about the data and the cluster model. Once you have the instance, you can try calculating a posterior for a given set of parameters. \n",
    "\n",
    "Now it's time to start completing the cluster class. Try out the log_prob function for parameter values both inside and outside the prior range - start with inside the prior range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:24:52.893350Z",
     "start_time": "2024-06-25T13:24:48.956547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob value:  1.0001667548882605\n",
      "time it took:  0.16797780990600586\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lens = Cluster(data_filename=filename_small)\n",
    "# logM200_prior_range = np.log10(np.linspace(1e14, 1e16, 100))  # in sun masses\n",
    "# c200_prior_range = np.linspace(2, 10, 100)  # dimensionless\n",
    "\n",
    "start = time()\n",
    "test = lens.log_prob(logM200=15, c200=7, normalize=True)\n",
    "end = time()\n",
    "\n",
    "print(\"logprob value: \", test)\n",
    "print(\"time it took: \", end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to check if your likelihood is fast enough - so you have to use the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:24:55.166901Z",
     "start_time": "2024-06-25T13:24:55.129056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Put your code here'"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Put your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a first look at the posterior for a range of parameter values. In order to do so, implement loops over $\\log M_{200}$ and $c_{200}$ with fixed step sizes. While you're at it, determine the parameters corresponding to the minimal $\\chi^2$. Determining the best fit parameters that way is called a grid search. \n",
    "\n",
    "Eventually on often chooses the same number of grid points for all parameters, however while developing your code, choose a different number of grid points for $\\log M_{200}$ and $c_{200}$. Index order is important and different library functions you use will have different conventions and this way you will get an error if you got it wrong.\n",
    "\n",
    "Plot your results to get a rough idea for the shape of $\\chi^2$. \n",
    "\n",
    "First make two plots: one where you fix $c_{200}$ to the best fit value and vary $\\log M_{200}$. And a second one where you fix $\\log M_{200}$ to the best fit value and vary $c_{200}$. This way you can varify that the $\\chi^2$ has the right shape and looks sensible. This is much harder to tell in a 2-d color plot.\n",
    "\n",
    "When you think everything looks good, make two 2-d plots, one for $\\chi^2$ and one for the likelihood. You may need to adjust parameter ranges for your parameters to identify the minimum/maximum visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:25:40.925372Z",
     "start_time": "2024-06-25T13:25:40.912457Z"
    }
   },
   "outputs": [],
   "source": [
    "class Grid_Search:\n",
    "    def __init__(self, func, n_x, n_y, par_min, par_max, p_names):\n",
    "        \"\"\"\n",
    "        Initialise arrays and variables and perform grid search.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        func: function for which the grid search is performed\n",
    "        n_x : number of grid steps of first parameter\n",
    "        n_y : number of grid steps of second parameter\n",
    "        par_min : list with minimum values of parameters (M200_min, c200_min)\n",
    "        par_max : list with maximum values of parameters (M200_max, c200_max)\n",
    "        p_names : list with parameter names (for plotting)\n",
    "        \"\"\"\n",
    "        x_min, y_min = par_min\n",
    "        x_max, y_max = par_max\n",
    "\n",
    "        x_edges = np.linspace(x_min, x_max, n_x+1)\n",
    "        y_edges = np.linspace(y_min, y_max, n_y+1)\n",
    "        \n",
    "        x_centers = 0.5*(x_edges[1:]+x_edges[:-1])\n",
    "        y_centers = 0.5*(y_edges[1:]+y_edges[:-1])\n",
    "        \n",
    "        chi_square_matrix = np.zeros((n_x, n_y))\n",
    "        \n",
    "        for i, x_val in enumerate(x_centers):\n",
    "            for j, y_val in enumerate(y_centers):\n",
    "                chi_square_matrix[i, j] = func(x_val, y_val)\n",
    "                \n",
    "        min_chi_square = np.min(chi_square_matrix)\n",
    "        i_min, j_min = np.where(chi_square_matrix == min_chi_square)\n",
    "        print(\"Optimal m_200 \", x_centers[i_min], \"Optimal c: \", y_centers[j_min])\n",
    "        chi_square_matrix = chi_square_matrix - chi_square_matrix[i_min, j_min]\n",
    "        \n",
    "        self.chi_sq = chi_square_matrix\n",
    "        self.x = x_centers\n",
    "        self.y = y_centers\n",
    "        self.ix_min = i_min\n",
    "        self.iy_min = j_min\n",
    "        self.extent = [x_edges[0], x_edges[-1], y_edges[0], y_edges[-1]]\n",
    "        self.p_names = p_names\n",
    "        self.gauss_fit_done = False\n",
    "\n",
    "    def plot1d(self, likelihood=False, axes=None, marginalize=False):\n",
    "        \"\"\"\n",
    "        Plot the 1d-chi square around the best fit parameters\n",
    "        \n",
    "        Optional Parameters:\n",
    "        --------------------\n",
    "        likelihood : [default: False] if true, plot the likelihood instead of the chi square\n",
    "        axes : pass an axes object if you'd like add the plot to an already existing one\n",
    "        marginalize: [default: False] if true, marginalize instead of plotting around the best fit parameters\n",
    "        \"\"\"\n",
    "        if axes is None:\n",
    "            _, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n",
    "\n",
    "        x = self.x\n",
    "        z = self.chi_sq\n",
    "        if likelihood:\n",
    "            z = np.exp(-0.5*z)\n",
    "        if marginalize:\n",
    "            z = np.sum(z, axis=1)\n",
    "            z /= np.mean(z)*np.ptp(x)\n",
    "        else:\n",
    "            z = z[:, self.iy_min]\n",
    "        axes[0].plot(x, z);\n",
    "        axes[0].set_xlabel(self.p_names[0]);\n",
    "        axes[0].set_ylabel(\"chi sq\");   \n",
    "        \n",
    "        y = self.y\n",
    "        z = self.chi_sq\n",
    "        if likelihood:\n",
    "            z = np.exp(-0.5*z)\n",
    "        if marginalize:\n",
    "            z = np.sum(z, axis=0)\n",
    "            z /= np.mean(z)*np.ptp(y)\n",
    "        else:\n",
    "            z = z[self.ix_min, :]\n",
    "        axes[1].plot(y, z);\n",
    "        axes[1].set_xlabel(self.p_names[1]);\n",
    "        axes[1].set_ylabel(\"chi sq\"); \n",
    "        \n",
    "        return axes\n",
    "    \n",
    "    def plot_gauss1d(self, axes=None):\n",
    "        \"\"\"\n",
    "        Plot the Gaussian best fit returned by the function get_gaussian_fit(). You have\n",
    "        to run get_gaussian_fit() first to use this function.\n",
    "        \n",
    "        Optional Parameters:\n",
    "        --------------------\n",
    "        axes : pass an axes object if you'd like add the plot to an already existing one\n",
    "        \"\"\"\n",
    "        if self.gauss_fit_done is False:\n",
    "            raise Exception(\"You have to run get_gaussian_fit() before you can call this function\")\n",
    "\n",
    "        if axes is None:\n",
    "            _, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n",
    "            \n",
    "        x = self.x\n",
    "        x = np.linspace(x.min(), x.max())\n",
    "        z = self.gaussian_fit(x, 0)\n",
    "        z /= np.sum(z)*np.ptp(x)/len(x)\n",
    "        axes[0].plot(x, z)\n",
    "\n",
    "        y = self.y\n",
    "        y = np.linspace(y.min(), y.max())\n",
    "        z = self.gaussian_fit(0, y)\n",
    "        z /= np.sum(z)*np.ptp(y)/len(y)\n",
    "        axes[1].plot(y, z)\n",
    "        \n",
    "        return axes\n",
    "        \n",
    "    def plot2d(self):\n",
    "        \"\"\"\n",
    "        Plot the 2d-chi square and likelihood\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n",
    "\n",
    "        raise Exception(\"plot2d is not implemented yet.\")\n",
    "        # TODO: Implement the plotting of the 2d chi square and likelihood\n",
    "        # remember that you have to transpose 2d arrays before plotting because\n",
    "        # of the different index ordering of arrays and matplotlib images\n",
    "\n",
    "        return axes\n",
    "        \n",
    "    def get_gaussian_fit(self, plot=False):\n",
    "        \"\"\"\n",
    "        Fit a Gaussian to the grid likelihood.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        f : chi square function corresponding to Gaussian best fit \n",
    "        g : Gaussian best fit function\n",
    "        \"\"\"\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "        z = self.chi_sq\n",
    "        \n",
    "        def h(p, x, y):\n",
    "            # helper function that returns a gaussian\n",
    "            a, b, da, db = p\n",
    "            aux = ((x-a)/da)**2 + ((y-b)/db)**2\n",
    "            res = np.exp(-0.5*aux)\n",
    "            return res\n",
    "\n",
    "        def err_func(p, x, y, z):\n",
    "            return h(p, x, y) - z\n",
    "\n",
    "        xv, yv = np.meshgrid(x, y, indexing='ij')\n",
    "        zv = z.flatten()\n",
    "        zv = np.exp(-0.5*zv)\n",
    "        xv = xv.flatten()\n",
    "        yv = yv.flatten()\n",
    "\n",
    "        p,_ = optimize.leastsq(err_func, [15, 2.6, 0.02, 0.2], args=(xv, yv, zv))\n",
    "\n",
    "        def f(x, y):\n",
    "            a, b, da, db = p\n",
    "            res = ((x-a)/da)**2 + ((y-b)/db)**2\n",
    "            return res\n",
    "\n",
    "        def g(x, y):\n",
    "            res = np.exp(-0.5*f(x,y))\n",
    "            return res\n",
    "\n",
    "        g.p = p\n",
    "        f.p = p\n",
    "        \n",
    "        if plot:\n",
    "            axes = self.plot1d(likelihood=True)\n",
    "            axes[0].plot(self.x, g(self.x, self.y[self.iy_min]))\n",
    "            axes[1].plot(self.y, g(self.x[self.ix_min], self.y))\n",
    "            \n",
    "        self.gaussian_fit = g\n",
    "        self.chi_sq_fit = f\n",
    "        self.gauss_fit_done = True\n",
    "        return f, g\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T13:28:35.388296Z",
     "start_time": "2024-06-25T13:25:47.028702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal m_200  [15.15] Optimal c:  [2.86666667]\n",
      "Time:  168.3574731349945\n"
     ]
    }
   ],
   "source": [
    "n_m, n_c = 60, 60 # for development use n_m!=n_c\n",
    "par_min = lens.par_min\n",
    "par_max = lens.par_max\n",
    "start = time()\n",
    "grid = Grid_Search(lens.log_prob, n_m, n_c, par_min, par_max, ['logM', 'c'])\n",
    "end = time()\n",
    "print(\"Time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:55:50.250094Z",
     "start_time": "2024-06-25T13:55:50.070269Z"
    }
   },
   "outputs": [],
   "source": [
    "chi_square_matrix = grid.chi_sq\n",
    "chi_square_slice_along_best_M = chi_square_matrix[17, :]\n",
    "chi_square_slice_along_best_c = chi_square_matrix[:, 3]\n",
    "\n",
    "c_range = grid.y\n",
    "M_range = grid.x\n",
    "\n",
    "plt.title(\"Sampled Likelihood along best $M_{200}$ value\")\n",
    "plt.plot(c_range, np.exp(-0.5*chi_square_slice_along_best_M))\n",
    "plt.xlabel(r\"$c$ values\")\n",
    "plt.ylabel(\"Probability values\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "\n",
    "plt.title(\"Sampled Likelihood along best $c$ value\")\n",
    "plt.plot(M_range, np.exp(-0.5*chi_square_slice_along_best_c))\n",
    "plt.xlabel(r\"$M_{200}$ values\")\n",
    "plt.ylabel(\"Probability values\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T13:15:14.647791Z",
     "start_time": "2024-06-25T13:15:14.508454Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(M_range, c_range)\n",
    "plt.contour(X, Y, chi_square_matrix.T, levels=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T13:15:25.377133Z",
     "start_time": "2024-06-25T13:15:25.343207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(M_range, c_range)\n",
    "cont = plt.contourf(X, Y, np.exp(-1/2*chi_square_matrix.T), levels=20)\n",
    "plt.colorbar(cont)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T15:42:26.982559Z",
     "start_time": "2024-06-18T15:42:26.778785Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Metropolis Hastings Algorithm</h2>\n",
    "\n",
    "Grid searches are robust, but not a very efficient way to determine the maximum likelihood (or posterior), especially for higher parametric problems. Gradient methods (generalizations of the Newton method) can efficiently search very big parameter spaces, but can - if one is interested in parameter errors - only be used in cases where the likelihood is sufficiently Gaussian and if no marginalization over nuisance parameters is desiered (nuisance parameters are parameters that are not of immediate interest to us, but will affect the errors of the parameters we're interested in). \n",
    "\n",
    "Markov chains can be used for small to medium sized parameter spaces to explore non-Gaussian likelihoods (i.e. cases where the likelihood has a complicated shape) and/or we need to marginalize over nuisance parameters (i.e. integrate the likelihood/posterior in certain dimensions).\n",
    "For these cases one can explore the likelihood/posterior with the help of Markov chains, as we will do in the next step.\n",
    "\n",
    "The `Sampler` class includes the functions to do a MCMC sampling and make some simple plots of the results. Again, the class is not complete yet. Look at the next few cells before you start coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T13:19:45.452113Z",
     "start_time": "2024-06-25T13:19:45.417445Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \n",
    "    def __init__(self, log_prob):\n",
    "        \"\"\"\n",
    "        Initialise Sampler\n",
    "        Parameters:\n",
    "        -----------\n",
    "        log_prob: function that provides a log-likelihood to be sampled\n",
    "        \"\"\"\n",
    "        self.log_prob = log_prob\n",
    "        \n",
    "\n",
    "    def run_mcmc(self, p0, cov, n_samples):\n",
    "        \"\"\"\n",
    "        Runs a Monte Carlo Markov Chain.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        p0         : initial parameters p=(M200_0, c200_0)\n",
    "        cov        : covariance matrix of the proposal distribution \n",
    "        n_samples  : number of mcmc samples  \n",
    "        \"\"\"    \n",
    "        log_prob0 = self.log_prob(*p0)\n",
    "        samples = np.zeros((len(p0), n_samples+1))\n",
    "        log_prob_arr = np.zeros(n_samples+1) # also store the values of the log-likelihood\n",
    "        \n",
    "        samples[:,0] = p0\n",
    "        for i in range(n_samples):\n",
    "            X = samples[:, i]\n",
    "            Y = np.random.multivariate_normal(mean=X, cov=cov, size=1)\n",
    "            U = np.random.uniform(low=0, high=1)\n",
    "            # alpha = min(1, self.log_prob(Y[0][0], Y[0][1])/self.log_prob(samples[:, i][0][0], samples[:, i][0][1]))\n",
    "            alpha = min(1, np.exp(-0.5*self.log_prob(*Y[0])+0.5*self.log_prob(*X)))\n",
    "            \n",
    "            if U <= alpha: \n",
    "                prob = self.log_prob(*X)\n",
    "                log_prob_arr[i] = prob\n",
    "                samples[:, i+1] = Y\n",
    "            else:\n",
    "                prob = self.log_prob(*samples[:, i])\n",
    "                log_prob_arr[i] = prob\n",
    "                samples[:, i+1] = X\n",
    "                \n",
    "        self.chain = samples\n",
    "        self.log_prob_arr = log_prob_arr\n",
    "    \n",
    "    def plot_chain(self, p_names=None, axes=None):\n",
    "        \"\"\"\n",
    "        Plots the chain\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        p_names (optional)      : provide parameter names for labeling the plot \n",
    "        other_chains (optional) : provide a list of chains from other samplers which \n",
    "                                  will be added to the plot\n",
    "        \"\"\"    \n",
    "        n_p = self.chain.shape[0]\n",
    "        n_rows = math.ceil(n_p/2)\n",
    "        dx = figsize[0]/2\n",
    "        dy = figsize[1]\n",
    "        n_cols = 2 if n_p > 1 else 1\n",
    "\n",
    "        if (p_names is not None) and (len(p_names) != n_p):\n",
    "            raise Exception(\"labels for parameters don't match number of parameters in chain\")\n",
    "        if axes is None:\n",
    "            _, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*dx, n_rows*dy))\n",
    "            axes[1].set_ylim(0,10)\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < n_p:\n",
    "                ax.plot(self.chain[i,:])\n",
    "                if p_names is not None:\n",
    "                    ax.set_ylabel(p_names[i])\n",
    "            else:\n",
    "                del ax\n",
    "        return axes\n",
    "                \n",
    "    def histogram(self, p_names=None, remove_burnin=0, axes=None):\n",
    "        \"\"\"\n",
    "        Make a histogram of the chain\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        p_names (optional)      : provide parameter names for labeling the plot \n",
    "        other_chains (optional) : provide a list of chains from other samplers which \n",
    "                                  will be added to the plot\n",
    "        remove_burnin (optional, default=0) : how many steps to discard from the chain\n",
    "        \"\"\"    \n",
    "        n_p = self.chain.shape[0]\n",
    "        n_rows = math.ceil(n_p/2)\n",
    "        dx = 9\n",
    "        dy = 6\n",
    "        n_cols = 2 if n_p > 1 else 1\n",
    "\n",
    "        if (p_names is not None) and (len(p_names) != n_p):\n",
    "            raise Exception(\"labels for parameters don't match number of parameters in chain\")\n",
    "        if axes is None:\n",
    "            _, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(n_cols*dx, n_rows*dy))\n",
    "        for i, ax in enumerate(axes.flatten()):\n",
    "            if i < n_p:\n",
    "                ax.hist(self.chain[i,remove_burnin:], histtype='step', bins=20, density=True)\n",
    "                if p_names is not None:\n",
    "                    ax.set_xlabel(p_names[i])\n",
    "            else:\n",
    "                del ax\n",
    "        return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to complete the Sampler class, create an instance and get some samples. \n",
    "\n",
    "However, calculating the likelihood is quite slow. For a few grid point it's reasonably fast, but if you want to run $10^5$samples with an MCMC you'll have to wait for some time to see how your results look. Therefore you won't develop your sampler code with the true likelihood, but we will use your grid search results to define an approximated function with almost the same shape. This will be much faster and almost as good. Once you're happy with your sampler and the widths of the proposal distribution you will use the real likelihood.\n",
    "\n",
    "You may have noticed a method get_gaussian_fit in the Grid_Search class. It is intended for just that purpose. Use it, to get the interpolated log likelihood for the grid search object you created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:26:30.910486Z",
     "start_time": "2024-06-25T14:26:30.777835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain:  [[14.         14.         14.         ... 14.99786121 14.99786121\n",
      "  14.99786121]\n",
      " [ 2.          2.          2.         ...  2.24964101  2.24964101\n",
      "   2.24964101]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9y/tnqj0p6s0mjcnrh409r__5l40000gn/T/ipykernel_13181/3462558954.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = min(1, np.exp(-0.5*self.log_prob(*Y[0])+0.5*self.log_prob(*X)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"Put the call of get_gaussian_fit here\"\n",
    "fitted_chi_sq_func, fitted_gaussian_func =  grid.get_gaussian_fit(plot=False)\n",
    "\n",
    "MCMCM_samples_approximate = Sampler(log_prob=fitted_chi_sq_func)\n",
    "chain = MCMCM_samples_approximate.run_mcmc(p0=[14,2.5], cov=np.array([[1, 0],[0, 0.01]]), n_samples=1000)\n",
    "print(\"Chain: \", MCMCM_samples_approximate.chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "array([<Axes: >, <Axes: >], dtype=object)"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCMCM_samples_approximate.plot_chain()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T14:26:31.101638Z",
     "start_time": "2024-06-25T14:26:31.021934Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Verify visually that the approximation worked by once again looking at the curves around the best fit values.\n",
    "Make four plots: one where you fix $c_{200}$ to the best fit value and vary $\\log M_{200}$. And a second one where you fix $\\log M_{200}$ to the best fit value and vary $c_{200}$. Make plots for both the $\\chi^2$ and the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T15:24:28.270510Z",
     "start_time": "2024-06-18T15:24:28.268976Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Put your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to run the sampler. Define a reasonable covariance. Do you have any idea what a __reasonably__ good numbers might be? Is there any way to find out from your previous results?\n",
    "\n",
    "No need to fine tune the covariance yet. That'll happen in the next step. So first, initialize and run your sampler, then make plots of the chains. Once those look like they are converging, plot the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-18T15:24:28.271996Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = Sampler(fitted_chi_sq_func)\n",
    "n_samples = 100\n",
    "p0 = my_cluster.get_random_parameter()\n",
    "cov = np.diag((1, 1))\n",
    "sampler.run_mcmc(p0, cov, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-18T15:24:28.275843Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Put your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your likelihood looks good, we'll take a look at how how different covariances affect the chain. Define three samplers and run them with different covariances. Make plots of the chains and histograms of the samples. Finally, make a 2d-plot of the likelihood to compare it to the plot you made for the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-18T15:24:28.278658Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Put your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once you have settled on a good covariance, it is time to run the MCMC for the __full__ data set on the __real__ likelihood, not the interpolated one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T15:24:28.281231Z",
     "start_time": "2024-06-18T15:24:28.280912Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Put your code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate at least 5 different chains and analyze them. Do not forget to include a check for convergence - add a method to the sampler class to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:22:08.446629Z",
     "start_time": "2024-06-25T15:19:00.949615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration\n",
      "Iteration\n",
      "Iteration\n",
      "Iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9y/tnqj0p6s0mjcnrh409r__5l40000gn/T/ipykernel_13181/3462558954.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  alpha = min(1, np.exp(-0.5*self.log_prob(*Y[0])+0.5*self.log_prob(*X)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Put your code here'"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "samplers = []\n",
    "c_range = np.linspace(2, 10, 5)\n",
    "M_range = np.linspace(14, 16, 5)\n",
    "for c0, M0 in zip(c_range, M_range):\n",
    "    print(\"Iteration\")\n",
    "    MCMCM_samples_real_likelhood = Sampler(log_prob=lens.log_prob)\n",
    "    MCMCM_samples_real_likelhood.run_mcmc(p0=[M0,c0], cov=np.array([[0.8, 0], [0, 1.2]]), n_samples=300)\n",
    "    samplers.append(MCMCM_samples_real_likelhood)\n",
    "\n",
    "\n",
    "\"Put your code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "array_of_mass_chains = []\n",
    "array_of_concentration_chains = []\n",
    "for sampler_instance in samplers:\n",
    "    array_of_mass_chains.append(sampler_instance.chain[0])\n",
    "    array_of_concentration_chains.append(sampler_instance.chain[1])\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(array_of_mass_chains[i], \".-\",)\n",
    "    \n",
    "\"\"\"for i in range(5):\n",
    "    plt.plot(array_of_concentration_chains[i], \".-\",)\"\"\"\n",
    "\n",
    "plt.ylim(0, 20)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T15:24:13.279866Z",
     "start_time": "2024-06-25T15:24:13.214546Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "def B(array_of_chains, M):\n",
    "    \"\"\"\n",
    "    :param chain: An array consisting of chains (arrays) values of M200's or c_200's\n",
    "    :param M:     Number of chains which are analyzed \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    N = len(array_of_chains[0])\n",
    "    array_of_means = np.array([np.mean(ch) for ch in array_of_chains])\n",
    "    mean_of_mean_array = np.mean(array_of_means)\n",
    "    return N/(M-1)*np.sum((array_of_means-mean_of_mean_array)**2)\n",
    "\n",
    "\n",
    "def W(array_of_chains, M):\n",
    "    N = len(array_of_chains[0])\n",
    "    array_of_means = np.array([np.mean(ch) for ch in array_of_chains])\n",
    "    bla = np.array([])\n",
    "    for i in range(M):\n",
    "        bla = np.append(bla, 1/(N-1) * np.sum((array_of_chains[i] - array_of_means[i])**2))\n",
    "    return 1/M * np.sum(bla)\n",
    "    \n",
    "    \n",
    "def R(M, array_of_chains):\n",
    "    N = len(array_of_chains[0])\n",
    "    var = (N-1)/N * W(array_of_chains, M) + 1/N*B(array_of_chains, M)\n",
    "    return var/W(array_of_chains, M)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T15:23:18.665456Z",
     "start_time": "2024-06-25T15:23:18.599622Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0795196003681156\n",
      "1.3303536036251677\n"
     ]
    }
   ],
   "source": [
    "convergence_factor_mass_chains = R(M=5, array_of_chains=array_of_mass_chains)\n",
    "convergence_concentration_chains = R(M=5, array_of_chains=array_of_concentration_chains)\n",
    "print(convergence_factor_mass_chains)\n",
    "print(convergence_concentration_chains)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T15:23:19.073183Z",
     "start_time": "2024-06-25T15:23:19.022682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
